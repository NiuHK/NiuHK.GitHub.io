<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>LLM 学习笔记 - HK&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content=" hsl(30, 100%, 50%)"><meta name="application-name" content="Daniel Niu"><meta name="msapplication-TileImage" content="img/icon1.svg"><meta name="msapplication-TileColor" content=" hsl(30, 100%, 50%)"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Daniel Niu"><meta name="apple-mobile-web-app-status-bar-style" content="default"><link rel="apple-touch-icon" sizes="72x72" href="img/icon1.svg"><link rel="apple-touch-icon" sizes="96x96" href="img/icon1.svg"><link rel="apple-touch-icon" sizes="128x128" href="img/icon1.svg"><link rel="apple-touch-icon" sizes="256x256" href="img/icon1.svg"><meta name="description" content="大型语言模型（llm）是一种特殊的预训练语言模型，通过调整模型大小、预训练语料库和计算来获得。由于llm规模大，对大量文本数据进行预训练，表现出特殊的能力，能够在许多自然语言处理任务中不经过任何特定任务训练的情况下取得显著的性能。llm的时代始于OpenAI的GPT-3模型，在ChatGPT和GPT4等模型的引入后，llm的流行程度呈指数级增长。LLM 表现出根据相对较少量的提示或输入做出预测的"><meta property="og:type" content="article"><meta property="og:title" content="LLM 学习笔记"><meta property="og:url" content="http://blog.212676.xyz/2024/03/12/LLM%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><meta property="og:site_name" content="HK&#039;s Blog"><meta property="og:description" content="大型语言模型（llm）是一种特殊的预训练语言模型，通过调整模型大小、预训练语料库和计算来获得。由于llm规模大，对大量文本数据进行预训练，表现出特殊的能力，能够在许多自然语言处理任务中不经过任何特定任务训练的情况下取得显著的性能。llm的时代始于OpenAI的GPT-3模型，在ChatGPT和GPT4等模型的引入后，llm的流行程度呈指数级增长。LLM 表现出根据相对较少量的提示或输入做出预测的"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://picstorage.212676.xyz/imgs/20240312221224.png"><meta property="og:image" content="https://picstorage.212676.xyz/imgs/20240312221223.png"><meta property="article:published_time" content="2024-03-12T14:15:16.000Z"><meta property="article:modified_time" content="2024-05-06T09:41:22.380Z"><meta property="article:author" content="Daniel Niu"><meta property="article:tag" content="LLM"><meta property="article:tag" content="AI"><meta property="article:tag" content="GPT"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://picstorage.212676.xyz/imgs/20240312221224.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://blog.212676.xyz/2024/03/12/LLM%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},"headline":"LLM 学习笔记","image":["https://picstorage.212676.xyz/imgs/20240312221224.png","https://picstorage.212676.xyz/imgs/20240312221223.png"],"datePublished":"2024-03-12T14:15:16.000Z","dateModified":"2024-05-06T09:41:22.380Z","author":{"@type":"Person","name":"Daniel Niu"},"publisher":{"@type":"Organization","name":"HK's Blog","logo":{"@type":"ImageObject","url":"http://blog.212676.xyz/2024/03/12/LLM%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/img/icon1.svg"}},"description":"大型语言模型（llm）是一种特殊的预训练语言模型，通过调整模型大小、预训练语料库和计算来获得。由于llm规模大，对大量文本数据进行预训练，表现出特殊的能力，能够在许多自然语言处理任务中不经过任何特定任务训练的情况下取得显著的性能。llm的时代始于OpenAI的GPT-3模型，在ChatGPT和GPT4等模型的引入后，llm的流行程度呈指数级增长。LLM 表现出根据相对较少量的提示或输入做出预测的"}</script><link rel="canonical" href="http://blog.212676.xyz/2024/03/12/LLM%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.1.1"><link rel="alternate" href="/atom.xml" title="HK's Blog" type="application/atom+xml">
</head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/icon1.svg" alt="HK&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/AboutMe">About</a></div><div class="navbar-end"><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-03-12T14:15:16.000Z" title="3/12/2024, 2:15:16 PM">2024-03-12</time></span></div></div><h1 class="title is-3 is-size-4-mobile">LLM 学习笔记</h1><div class="content"><blockquote>
<p>大型语言模型（llm）是一种特殊的预训练语言模型，通过调整模型大小、预训练语料库和计算来获得。由于llm规模大，对大量文本数据进行预训练，表现出特殊的能力，能够在许多自然语言处理任务中不经过任何特定任务训练的情况下取得显著的性能。<br>llm的时代始于OpenAI的GPT-3模型，在ChatGPT和GPT4等模型的引入后，llm的流行程度呈指数级增长。<br>LLM 表现出根据相对较少量的提示或输入做出预测的非凡能力。LLM 可用于生成式人工智能，以根据采用人类语言的输入提示生成内容。</p>
</blockquote>
<span id="more"></span>
<h1 id="基于transformer架构的LLM："><a href="#基于transformer架构的LLM：" class="headerlink" title="基于transformer架构的LLM："></a>基于transformer架构的LLM：</h1><p>自注意力机制，关注词和当前输入序列的所有词的关系，提高训练速度</p>
<p><img src="https://picstorage.212676.xyz/imgs/20240312221224.png"></p>
<h2 id="文本token化"><a href="#文本token化" class="headerlink" title="文本token化"></a>文本token化</h2><p>每个被一个整数表示</p>
<p>传入嵌入层，每个token被一个向量表示（词向量），向量空间中可以表示更多相关性（多维度）</p>
<h2 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h2><p>代表词位置的位置向量与词向量组合，使模型同时理解词的意义以及在句中的关系</p>
<h2 id="编码器（多级串联-更深入了解）"><a href="#编码器（多级串联-更深入了解）" class="headerlink" title="编码器（多级串联-&gt;更深入了解）"></a>编码器（多级串联-&gt;更深入了解）</h2><p>多头自注意力机制-&gt; 词之间相关性表示权重</p>
<blockquote>
<p>有多头自注意力机制，用来关注文本不同特征方面（动词、名词），并行运算互不影响，每个头的权重是训练中调整的。</p>
</blockquote>
<p>位置编码的向量传入，加入自注意力机制的权重表示，输出（三种信息融合在向量里）向量。（同一个词，上下文不同表示不同）</p>
<p>前馈神经网络，增加模型表达能力</p>
<h2 id="解码器（生成）"><a href="#解码器（生成）" class="headerlink" title="解码器（生成）"></a>解码器（生成）</h2><p>一个特殊值（便于考虑之前已经生成的上文保持上下文连贯性）已生成的输出序列经过另一编码器，但使用 <code>带掩码的多头自注意力机制</code>，即自注意力头只关注当前词和其之前的词的关系权重，</p>
<blockquote>
<p>确保解码器生成文本遵循正确时间顺序</p>
</blockquote>
<p>编码器传出的向量（token的抽象表示）-&gt;多头自注意力</p>
<p>用来捕捉解码器即将产生成的输出以及编码器输入之间的关系，使输入序列信息融合到解码器输出中</p>
<p>前馈神经网络，增加模型表达能力</p>
<p><img src="https://picstorage.212676.xyz/imgs/20240312221223.png"></p>
<p>线性层，softmax层，转换解码器输出到词汇表概率分布（实现猜下一个词）</p>
<h2 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h2><p>仅解码器：掩码语言建模、情感分析 BERT</p>
<p>仅解码器：猜测上下文文本生成 GPT2、3</p>
<p>解码器解码器模型&#x2F; 序列到序列模型：翻译总结  T5、bart</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>LLM 学习笔记</p><p><a href="http://blog.212676.xyz/2024/03/12/LLM学习笔记/">http://blog.212676.xyz/2024/03/12/LLM学习笔记/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Daniel Niu</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2024-03-12</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2024-05-06</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/LLM/">LLM</a><a class="link-muted mr-2" rel="tag" href="/tags/AI/">AI</a><a class="link-muted mr-2" rel="tag" href="/tags/GPT/">GPT</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2024/03/17/%E8%B4%B9%E6%9B%BC%E6%8A%80%E5%B7%A7/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">费曼技巧</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2024/03/11/sym/"><span class="level-item">sym</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "91793c8904eb75cd432fc77bebf26c5d",
            repo: "NiuHK.github.io",
            owner: "NiuHK",
            clientID: "033643d93760c7549ef5",
            clientSecret: "b082f7d7117bbd41d66e0176376cd9b09bb36adb",
            admin: ["NiuHK"],
            createIssueManually: false,
            distractionFreeMode: true,
            perPage: 20,
            pagerDirection: "last",
            
            
            enableHotKey: true,
            language: "zh-CN",
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/head.png" alt="Daniel Niu"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Daniel Niu</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1" style="color: hsl(30, 100%, 50%)"></i><span>CN</span><h2 class="hitokoto" id="yiyancon" style="color: #0a0a0a"></h2><script type="text/javascript" src="https://international.v1.hitokoto.cn?encode=js&amp;amp;charset=utf-8&amp;amp;&amp;amp;c=j&amp;amp;c=k&amp;amp;c=l"></script></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">23</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">0</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">26</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/NiuHK" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/NiuHK"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#基于transformer架构的LLM："><span class="level-left"><span class="level-item">1</span><span class="level-item">基于transformer架构的LLM：</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#文本token化"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">文本token化</span></span></a></li><li><a class="level is-mobile" href="#位置编码"><span class="level-left"><span class="level-item">1.2</span><span class="level-item">位置编码</span></span></a></li><li><a class="level is-mobile" href="#编码器（多级串联-更深入了解）"><span class="level-left"><span class="level-item">1.3</span><span class="level-item">编码器（多级串联-&gt;更深入了解）</span></span></a></li><li><a class="level is-mobile" href="#解码器（生成）"><span class="level-left"><span class="level-item">1.4</span><span class="level-item">解码器（生成）</span></span></a></li><li><a class="level is-mobile" href="#扩展"><span class="level-left"><span class="level-item">1.5</span><span class="level-item">扩展</span></span></a></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/icon1.svg" alt="HK&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2024 Daniel Niu</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p></div></div></div></div></footer><script src="/js/custom.js" defer></script><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>